{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2eL_MNB3yIu",
        "outputId": "9ffcea33-fc35-4252-f555-591878719840"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MetadataÔºåEmbeddingÔºåCosSim(NSAI)\n",
        "\n",
        "# pip install openai scikit-learn pandas numpy\n",
        "\n",
        "import os, json, hashlib, getpass\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from openai import OpenAI\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "csv_folder  = \"/content/drive/MyDrive/Geochem_data_060\"\n",
        "json_folder = \"/content/drive/MyDrive/Geochem_Knowledge_060\"\n",
        "\n",
        "# Metadata embedding\n",
        "META_COLS = [\n",
        "    \"SYSTEM_TYPE\",\n",
        "    \"PRIMARY_CLASS\", \"SECONDARY_CLASS\", \"SPECIFIC_NAME\",\n",
        "    \"HOST_ROCK_TYPE\",\n",
        "    \"STRATIGRAPHY\",\n",
        "    \"MINERALIZATION\",\n",
        "    \"ALTERATION\",\n",
        "    \"IGNEOUS_FORM\",\n",
        "    \"METAMORPHISM\", \"FACIES_GRADE\",\n",
        "    \"GEOLOGIC_AGE\", \"GEOLOGIC_AGE_DEPOSIT\", \"GEOLOGIC_AGE_HOST\",\n",
        "    \"HOST_NAME\",\n",
        "]\n",
        "\n",
        "\n",
        "NUMERIC_COLS = None\n",
        "\n",
        "MAX_PER_CLASS = 200\n",
        "\n",
        "# Embedding cache\n",
        "KB_CACHE_PATH     = \"kb_embedding_cache.json\"\n",
        "SAMPLE_CACHE_PATH = \"sample_meta_embedding_cache.json\"\n",
        "\n",
        "# OpenAI embedding\n",
        "EMB_MODEL = \"text-embedding-3-small\"\n",
        "EMB_DIM   = 1536\n",
        "\n",
        "RF_PARAMS = dict(\n",
        "    n_estimators=600,\n",
        "    max_depth=None,\n",
        "    min_samples_split=5,\n",
        "    max_features=\"sqrt\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "api_key = getpass.getpass(\"üîê Enter your OpenAI API key: \")\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def sha16(s: str) -> str:\n",
        "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()[:16]\n",
        "\n",
        "def load_cache(path: str) -> dict:\n",
        "    if os.path.exists(path):\n",
        "        try:\n",
        "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return json.load(f)\n",
        "        except Exception:\n",
        "            return {}\n",
        "    return {}\n",
        "\n",
        "def save_cache(path: str, obj: dict):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(obj, f, ensure_ascii=False)\n",
        "\n",
        "def get_embedding(text: str) -> list:\n",
        "    text = (text or \"\").replace(\"\\n\", \" \").strip()\n",
        "    if not text:\n",
        "        return [0.0] * EMB_DIM\n",
        "    return client.embeddings.create(input=[text], model=EMB_MODEL).data[0].embedding\n",
        "\n",
        "def safe_load_json_as_text(path: str) -> str:\n",
        "    raw = open(path, \"r\", encoding=\"utf-8\").read().strip()\n",
        "    if not raw:\n",
        "        return \"\"\n",
        "    try:\n",
        "        obj = json.loads(raw)\n",
        "        return json.dumps(obj, ensure_ascii=False)\n",
        "    except Exception:\n",
        "        return raw\n",
        "\n",
        "def cosine_sim(A: np.ndarray, B: np.ndarray) -> np.ndarray:\n",
        "    # A: (n,d), B: (m,d) -> (n,m)\n",
        "    A = A / (np.linalg.norm(A, axis=1, keepdims=True) + 1e-12)\n",
        "    B = B / (np.linalg.norm(B, axis=1, keepdims=True) + 1e-12)\n",
        "    return A @ B.T\n",
        "\n",
        "# load data\n",
        "\n",
        "def load_all_csv(csv_dir: str) -> pd.DataFrame:\n",
        "    frames = []\n",
        "    for fn in os.listdir(csv_dir):\n",
        "        if fn.lower().endswith(\".csv\"):\n",
        "            df = pd.read_csv(os.path.join(csv_dir, fn))\n",
        "            if \"DEPOSIT_TYPE\" in df.columns:\n",
        "                frames.append(df)\n",
        "    if not frames:\n",
        "        raise FileNotFoundError(f\"No CSV with DEPOSIT_TYPE found in {csv_dir}\")\n",
        "    data = pd.concat(frames, ignore_index=True)\n",
        "    data[\"DEPOSIT_TYPE\"] = data[\"DEPOSIT_TYPE\"].astype(str)\n",
        "    return data\n",
        "\n",
        "def downsample_per_class(df: pd.DataFrame, max_per_class: int | None) -> pd.DataFrame:\n",
        "    if max_per_class is None:\n",
        "        return df\n",
        "    return (df.groupby(\"DEPOSIT_TYPE\", group_keys=False)\n",
        "              .apply(lambda x: x.sample(n=min(len(x), max_per_class), random_state=42))\n",
        "              .reset_index(drop=True))\n",
        "\n",
        "def infer_numeric_cols(df: pd.DataFrame, meta_cols: list[str]) -> list[str]:\n",
        "    drop_cols = set([\"DEPOSIT_TYPE\"] + meta_cols)\n",
        "    numeric_cols = []\n",
        "    for c in df.columns:\n",
        "        if c in drop_cols:\n",
        "            continue\n",
        "        if pd.api.types.is_numeric_dtype(df[c]):\n",
        "            numeric_cols.append(c)\n",
        "    return numeric_cols\n",
        "\n",
        "# deposit knowledge embeddings\n",
        "\n",
        "def build_kb_embeddings(json_dir: str) -> tuple[list[str], np.ndarray]:\n",
        "    cache = load_cache(KB_CACHE_PATH)\n",
        "    names = []\n",
        "    embs  = []\n",
        "\n",
        "    for fn in os.listdir(json_dir):\n",
        "        if not fn.lower().endswith(\".json\"):\n",
        "            continue\n",
        "        deposit_type = fn.replace(\"_knowledge.json\", \"\").strip()\n",
        "        text = safe_load_json_as_text(os.path.join(json_dir, fn))\n",
        "        key = f\"{deposit_type}|{sha16(text)}\"\n",
        "        if key not in cache:\n",
        "            cache[key] = get_embedding(text)\n",
        "        names.append(deposit_type)\n",
        "        embs.append(np.array(cache[key], dtype=float))\n",
        "\n",
        "    save_cache(KB_CACHE_PATH, cache)\n",
        "\n",
        "    if not names:\n",
        "        raise FileNotFoundError(f\"No .json found in {json_dir}\")\n",
        "\n",
        "    order = np.argsort(names)\n",
        "    names = [names[i] for i in order]\n",
        "    embs  = np.vstack([embs[i] for i in order])\n",
        "    return names, embs\n",
        "\n",
        "\n",
        "\n",
        "def row_to_meta_text(row: pd.Series, meta_cols: list[str]) -> str:\n",
        "    banned = {\"DEPOSIT_TYPE\"}\n",
        "    parts = []\n",
        "\n",
        "    for c in meta_cols:\n",
        "        if c in banned:\n",
        "            continue\n",
        "        if c not in row.index:\n",
        "            continue\n",
        "\n",
        "        v = row[c]\n",
        "        if pd.isna(v):\n",
        "            continue\n",
        "\n",
        "        s = str(v).strip()\n",
        "        if not s or s.lower() in {\"nan\", \"none\", \"null\"}:\n",
        "            continue\n",
        "\n",
        "        s = \" \".join(s.split())\n",
        "        s_norm = s.lower()\n",
        "\n",
        "        parts.append(f\"{c.lower()}: {s_norm}\")\n",
        "\n",
        "    if not parts:\n",
        "        return \"\"\n",
        "    return \" | \".join(parts)\n",
        "\n",
        "\n",
        "def build_sample_meta_embeddings(df: pd.DataFrame, meta_cols: list[str]) -> np.ndarray:\n",
        "    cache = load_cache(SAMPLE_CACHE_PATH)\n",
        "    out = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        text = row_to_meta_text(df.iloc[i], meta_cols)\n",
        "        key = sha16(text)\n",
        "        if key not in cache:\n",
        "            cache[key] = get_embedding(text)\n",
        "        out.append(np.array(cache[key], dtype=float))\n",
        "\n",
        "    save_cache(SAMPLE_CACHE_PATH, cache)\n",
        "    return np.vstack(out)\n",
        "\n",
        "# drop-in variants for K\n",
        "\n",
        "def permute_rows(K: np.ndarray, seed: int) -> np.ndarray:\n",
        "    rng = np.random.RandomState(seed)\n",
        "    return K[rng.permutation(len(K))]\n",
        "\n",
        "def col_shuffle(K: np.ndarray, seed: int) -> np.ndarray:\n",
        "    rng = np.random.RandomState(seed)\n",
        "    K2 = K.copy()\n",
        "    for j in range(K.shape[1]):\n",
        "        K2[:, j] = K2[rng.permutation(len(K2)), j]\n",
        "    return K2\n",
        "\n",
        "\n",
        "def fit_eval_rf(Xtr, ytr, Xte, yte, seed: int):\n",
        "    clf = RandomForestClassifier(random_state=seed, **RF_PARAMS)\n",
        "    clf.fit(Xtr, ytr)\n",
        "    pred = clf.predict(Xte)\n",
        "    return {\n",
        "        \"acc\": accuracy_score(yte, pred),\n",
        "        \"macro_f1\": f1_score(yte, pred, average=\"macro\"),\n",
        "        \"weighted_f1\": f1_score(yte, pred, average=\"weighted\"),\n",
        "    }\n",
        "\n",
        "def summarize(metrics_list):\n",
        "    arr = {k: np.array([m[k] for m in metrics_list], float) for k in [\"acc\", \"macro_f1\", \"weighted_f1\"]}\n",
        "    return {k: (arr[k].mean(), arr[k].std(ddof=1)) for k in arr.keys()}\n",
        "\n",
        "# MAIN\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_all_csv(csv_folder)\n",
        "    df = downsample_per_class(df, MAX_PER_CLASS)\n",
        "\n",
        "    for c in META_COLS:\n",
        "        if c not in df.columns:\n",
        "            print(f\"‚ö†Ô∏è META_COL '{c}' not found in df columns. It will be treated as missing for all rows.\")\n",
        "\n",
        "    y = df[\"DEPOSIT_TYPE\"].values\n",
        "    if NUMERIC_COLS is None:\n",
        "        numeric_cols = infer_numeric_cols(df, META_COLS)\n",
        "    else:\n",
        "        numeric_cols = NUMERIC_COLS\n",
        "\n",
        "    print(f\"Samples={len(df)} | Classes={pd.Series(y).nunique()} | Numeric={len(numeric_cols)} | MetaCols={META_COLS}\")\n",
        "\n",
        "    # 1) KB embeddings\n",
        "    kb_names, kb_emb = build_kb_embeddings(json_folder)\n",
        "    print(f\"KB types loaded: {len(kb_names)}\")\n",
        "\n",
        "    # 2) Sample metadata embeddings\n",
        "    print(\"Building sample metadata embeddings (once) ‚Ä¶\")\n",
        "    S_meta = build_sample_meta_embeddings(df, META_COLS)  # (n,1536)\n",
        "\n",
        "    # 3) K_all\n",
        "    K_all = cosine_sim(S_meta, kb_emb)  # (n, C)\n",
        "\n",
        "    # 4) 10 seeds quick validation\n",
        "    seeds = list(range(10))\n",
        "    res = {k: [] for k in [\"baseline\", \"nsai\", \"drop_perm\", \"drop_col\", \"drop_zero\"]}\n",
        "\n",
        "    idx_all = np.arange(len(df))\n",
        "    for seed in seeds:\n",
        "        tr_idx, te_idx = train_test_split(idx_all, test_size=0.2, random_state=seed, stratify=y)\n",
        "\n",
        "        # y encode (train only)\n",
        "        le = LabelEncoder()\n",
        "        ytr = le.fit_transform(y[tr_idx])\n",
        "\n",
        "        yte_raw = y[te_idx]\n",
        "        mask = np.isin(yte_raw, le.classes_)\n",
        "        te_idx2 = te_idx[mask]\n",
        "        yte = le.transform(y[te_idx2])\n",
        "\n",
        "        Xtr_raw = df.iloc[tr_idx][numeric_cols].values\n",
        "        Xte_raw = df.iloc[te_idx2][numeric_cols].values\n",
        "\n",
        "        imputer = SimpleImputer(strategy=\"mean\")\n",
        "        scaler  = StandardScaler()\n",
        "\n",
        "        Xtr_imp = imputer.fit_transform(Xtr_raw)\n",
        "        Xte_imp = imputer.transform(Xte_raw)\n",
        "\n",
        "        Xtr_num = scaler.fit_transform(Xtr_imp)\n",
        "        Xte_num = scaler.transform(Xte_imp)\n",
        "\n",
        "        # ---- K (label-free) ----\n",
        "        Ktr = K_all[tr_idx, :]      # (n_train, C_all)\n",
        "        Kte = K_all[te_idx2, :]\n",
        "\n",
        "        # baseline\n",
        "        res[\"baseline\"].append(fit_eval_rf(Xtr_num, ytr, Xte_num, yte, seed))\n",
        "\n",
        "        # nsai = numeric + K\n",
        "        res[\"nsai\"].append(fit_eval_rf(np.hstack([Xtr_num, Ktr]), ytr, np.hstack([Xte_num, Kte]), yte, seed))\n",
        "\n",
        "        # strict drop-in 1: permute rows in train+test independently\n",
        "        res[\"drop_perm\"].append(\n",
        "            fit_eval_rf(\n",
        "                np.hstack([Xtr_num, permute_rows(Ktr, seed)]), ytr,\n",
        "                np.hstack([Xte_num, permute_rows(Kte, seed + 100)]), yte,\n",
        "                seed\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # strict drop-in 2: column-wise shuffle train+test\n",
        "        res[\"drop_col\"].append(\n",
        "            fit_eval_rf(\n",
        "                np.hstack([Xtr_num, col_shuffle(Ktr, seed)]), ytr,\n",
        "                np.hstack([Xte_num, col_shuffle(Kte, seed + 100)]), yte,\n",
        "                seed\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # drop-in 3: zero K\n",
        "        res[\"drop_zero\"].append(\n",
        "            fit_eval_rf(\n",
        "                np.hstack([Xtr_num, np.zeros_like(Ktr)]), ytr,\n",
        "                np.hstack([Xte_num, np.zeros_like(Kte)]), yte,\n",
        "                seed\n",
        "            )\n",
        "        )\n",
        "\n",
        "    print(\"\\n===== SUMMARY (mean ¬± std, 10 seeds) =====\")\n",
        "    for key in [\"baseline\", \"nsai\", \"drop_perm\", \"drop_col\", \"drop_zero\"]:\n",
        "        s = summarize(res[key])\n",
        "        print(f\"\\n{key.upper()}\")\n",
        "        print(f\"  Acc     : {s['acc'][0]:.3f} ¬± {s['acc'][1]:.3f}\")\n",
        "        print(f\"  MacroF1 : {s['macro_f1'][0]:.3f} ¬± {s['macro_f1'][1]:.3f}\")\n",
        "        print(f\"  WeightF1: {s['weighted_f1'][0]:.3f} ¬± {s['weighted_f1'][1]:.3f}\")\n",
        "\n",
        "    i0 = 0\n",
        "    topk = 5\n",
        "    sims = K_all[i0]\n",
        "    order = np.argsort(-sims)[:topk]\n",
        "    print(f\"\\n[Sanity check] Sample 0 meta text = {row_to_meta_text(df.iloc[i0], META_COLS)}\")\n",
        "    print(\"Top-5 KB similarity:\")\n",
        "    for j in order:\n",
        "        print(f\"  {kb_names[j]} : {sims[j]:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cqIaXyQQhln7",
        "outputId": "4584233f-fe45-4044-99a5-ed643f05b309"
      },
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîê Enter your OpenAI API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3147764017.py:128: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda x: x.sample(n=min(len(x), max_per_class), random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples=2281 | Classes=17 | Numeric=76 | MetaCols=['SYSTEM_TYPE', 'PRIMARY_CLASS', 'SECONDARY_CLASS', 'SPECIFIC_NAME', 'HOST_ROCK_TYPE', 'STRATIGRAPHY', 'MINERALIZATION', 'ALTERATION', 'IGNEOUS_FORM', 'METAMORPHISM', 'FACIES_GRADE', 'GEOLOGIC_AGE', 'GEOLOGIC_AGE_DEPOSIT', 'GEOLOGIC_AGE_HOST', 'HOST_NAME']\n",
            "KB types loaded: 17\n",
            "Building sample metadata embeddings (once) ‚Ä¶\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: [52 53]. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== SUMMARY (mean ¬± std, 10 seeds) =====\n",
            "\n",
            "BASELINE\n",
            "  Acc     : 0.979 ¬± 0.007\n",
            "  MacroF1 : 0.969 ¬± 0.011\n",
            "  WeightF1: 0.979 ¬± 0.007\n",
            "\n",
            "NSAI\n",
            "  Acc     : 0.984 ¬± 0.004\n",
            "  MacroF1 : 0.973 ¬± 0.011\n",
            "  WeightF1: 0.984 ¬± 0.005\n",
            "\n",
            "DROP_PERM\n",
            "  Acc     : 0.977 ¬± 0.007\n",
            "  MacroF1 : 0.965 ¬± 0.013\n",
            "  WeightF1: 0.977 ¬± 0.007\n",
            "\n",
            "DROP_COL\n",
            "  Acc     : 0.978 ¬± 0.008\n",
            "  MacroF1 : 0.967 ¬± 0.012\n",
            "  WeightF1: 0.977 ¬± 0.008\n",
            "\n",
            "DROP_ZERO\n",
            "  Acc     : 0.978 ¬± 0.006\n",
            "  MacroF1 : 0.968 ¬± 0.010\n",
            "  WeightF1: 0.978 ¬± 0.006\n",
            "\n",
            "[Sanity check] Sample 0 meta text = system_type: magmatic ree | primary_class: mineral | specific_name: calcite | host_rock_type: carbonatite | geologic_age_host: eocene | host_name: bear lodge carbonatite\n",
            "Top-5 KB similarity:\n",
            "  Carbonatite : 0.559\n",
            "  Polymetallic sulfide skarn : 0.520\n",
            "  Polymetallic sulfide skarn_replacement : 0.515\n",
            "  High sulfidation Au-Ag : 0.482\n",
            "  Porphyry Cu (Au) : 0.481\n"
          ]
        }
      ]
    }
  ]
}